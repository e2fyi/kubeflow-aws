# Kubeflow Pipelines

[![0.5.1](https://img.shields.io/badge/kubeflow--pipelines-0.5.1-blue.svg)](https://github.com/kubeflow/pipelines/tree/0.5.1)

[Kubeflow pipelines (kfp)](https://github.com/kubeflow/pipelines) are reusable end-to-end ML workflows.
As of release `0.5.1`, kfp comprises of the following components:

| service                                                    | description                                                                                                                                                                                                    |
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`argo`](https://github.com/argoproj/argo)                 | main workhorse to run the pipelines                                                                                                                                                                            |
| `ml-pipeline-apiserver`                                    | primary endpoint to interact with kubeflow pipelines APIs (e.g. get runs, experiments, etc)                                                                                                                    |
| `ml-pipeline-persistentagent`                              | multiple workers that performs various misc tasks - e.g. collect metrics, save resources to db, etc                                                                                                            |
| `ml-pipeline-scheduledworkflow`                            | controller to schedule workflows                                                                                                                                                                               |
| `ml-pipeline-ui`                                           | serves the React frontend and nodejs backend - i.e. webapp for users to interact with pipelines                                                                                                                |
| `ml-pipeline-viewer-crd`                                   | controller to manage viewers - i.e. pods serving tensorboard                                                                                                                                                   |
| `ml-pipeline-visualizationserver`                          | endpoints to generate and serve visualizations for pipelines                                                                                                                                                   |
| [`metadata-server`](https://github.com/google/ml-metadata) | endpoints to record and retrieve metadata associated with the workflows                                                                                                                                        |
| `metadata-writer` | controller to help write metadata for workflows without needing any additional codes in the sdk side.                                                                                                                                       |
| `mysql`                                                    | primary database to store most of the pipeline resources and information                                                                                                                                       |
| `minio`                                                    | `minio` can either be deployed as a standalone object store service, or as a gateway to object store services like S3. This manifest will not deploy `minio` service as we will be connecting to `s3` directly |
| `s3`                                                       | `s3` will be used as the primary storage for workflow templates and artifacts generated by the pipelines (i.e. logs, metrics, ui-metrics, intermediate data, etc)                                              |
| `cache`                                                    | cache intermediate execution results |
| `cache-deployer`                                           | cache deployer |

> In addition, [kfp sdk](https://pypi.org/project/kfp/) is a python package that can be used to create reusable ml workflows.

## Quick start

> Please change the passwords inside `mysql-secrets.env`.

```bash
# generate the Cluster Scoped Resources for kubeflow pipelines (first time only)
kubectl kustomize cluster-scoped-resources > kubeflow-pipelines-csr.yaml
# deploy the cluster-scoped-resources
kubectl apply -f kubeflow-pipelines-csr.yaml
# generate the provided overlay variant
kubectl kustomize overlay/${VARIANT} > kubeflow-pipelines-aws.yaml
# deploy
kubectl apply -f kubeflow-pipelines-aws.yaml
```

## Kustomize

These manifests are written with [kustomize](https://github.com/kubernetes-sigs/kustomize) in mind. `kubectl@1.14` and above natively supports `kustomize`.

To generate the deployment YAMLS to stdout:

```bash
# generates manifest for access key based approach
kubectl kustomize overlay/acccesskey
# generates manifest for iam approach
kubectl kustomize overlay/iam
```

To deploy:

Create `kubeflow-pipelines` custom resource definitions (crd):
```bash
# generate crd yaml
kubectl kustomize crds > kubeflow-crds.yaml
# apply yaml
kubectl apply -f kubeflow-crds.yaml
```

Deploy `kubeflow-pipelines`:
```bash
# generate yaml
kubectl kustomize overlay/${VARIANT} > kubeflow-pipelines-aws.yaml
# apply yaml
kubectl apply -f kubeflow-pipelines-aws.yaml
```

To uninstall:

```bash
# delete all k8s resources specified in yaml
kubectl delete -f kubeflow-pipelines-aws.yaml
```

```bash
# delete all k8s crds associated with kubeflow pipelines
kubctl delete -f kubeflow-crds.yaml
```

> ### IMPORTANT NOTE
>
> Be careful when deleting k8s resources. Cluster-level resources such as CRD,
> cluster roles, cluster role-bindings, etc, are shared by all kubeflow pipelines
> deployments in different namespaces.

## Folder structure

`crds` folder references the crds manifest in the kubeflow pipelines repo so they can be deployed
separately. The reason for this is `crd` is cluster resource and shared by multiple namespaces.
If they are merged as a single `kustomize` deployment, it will potentially affect other
deployments when we destroy. This is to help users that want to deploy multiple kubeflow pipelines
in different namespaces.

`base` folder references the light-weight pipeline manifest in the kubeflow
pipelines repo, as well as a basic mysql manifest from the main kubeflow manifest
repo. It configures argo to use s3 bucket as the default artifactory. You can
find most of the basic configurations to use S3 here, whereas the overlay
provides further customization on providing permissions to access S3.

`overlay` folder has 2 variants `accesskey` and `iam`. `accesskey` provides the
manifest to deploy a kubeflow pipelines deployment that interact with S3 using
the AWS access key provided in a user generated k8s secret. `iam` provides the
manifest to deploy a kubeflow pipeline deployment that manage AWS credentials
with pod annotations - i.e. [kube2iam](https://github.com/jtblin/kube2iam) or
[iam role for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)
is configured for your cluster.

## Notable changes from official manifest

> ### NOTE
>
> The folder (defaults to `pipelines`) to save the pipeline templates cannot be
> configured until
> [#2080](https://github.com/kubeflow/pipelines/pull/2080) is merged in.

#### Archiving pod logs

`archiveLogs` is set to `true` so that pod logs are automatically archived into
the configured S3 bucket. `ml-pipeline-ui` will be able to retrieve the pod logs
even if the pod (and node) had been removed and purged.

#### MySQL

For simplicity, a generic mysql 5.6 service is provisioned. This can be replaced
with AWS Aurora or MYSQL RDS if needed (but not tested).

#### Access-key based access

A k8s secret with the AWS credential must be created. This secret will be referenced
by the various kfp services to access the S3 buckets.

```bash
kubectl -n kubeflow create secret generic ml-pipeline-aws-secret \
    --from-literal=accesskey=$AWS_ACCESS_KEY_ID \
    --from-literal=secretkey=$AWS_SECRET_ACCESS_KEY
```

#### IAM based access

You can annotate the manifest with the required IAM annotations using the
`commonAnnotations` instruction. Moreover, you will also need to update `params.env`
with the corresponding annotation key and value. This is used to update the
tensorboard pod template spec (as `commonAnnotations` does not insert annotations
to the json string). Alternatively, you can just edit the templat in the configmap
directly.

An appropriate IAM role must be also created for kfp services to access the S3 buckets.

This approach assumes that an IAM credential provisioning service
(e.g. [kube2iam](https://github.com/jtblin/kube2iam)) is deployed in the k8s cluster.

Alternatively, existing IAM role that is assigned to the k8s nodes can be updated to
permit access to the appropriate S3 buckets (not recommended).

You can also setup IAM roles for your k8s service account (see [AWS docs](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html))
